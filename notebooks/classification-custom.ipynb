{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook trains a classification model to predict 'tags' or 'severity' for the given Project Sidewalk label type."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0XmqhrS1VF2g",
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:18.445670Z",
     "start_time": "2024-04-03T15:45:14.746548Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "from torchvision import transforms, io\n",
    "from dinov2.models.vision_transformer import vit_small, vit_base, vit_large, vit_giant2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# some initial variables\n",
    "\n",
    "local_directory = os.getcwd()\n",
    "\n",
    "# Enum for the classification categories we support.\n",
    "C12N_CATEGORY = {\n",
    "    'TAGS': 'tags',\n",
    "    'SEVERITY': 'severity'\n",
    "}\n",
    "\n",
    "# this has to be consistent with the data generation script\n",
    "c12n_category_offset = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:18.450421Z",
     "start_time": "2024-04-03T15:45:18.447254Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# All the parameters that need to be configured for a training run should be in this cell.\n",
    "# Everywhere else we will use these variables.\n",
    "\n",
    "# classification category. currently, one of 'severity' or 'tags'.\n",
    "c12n_category = C12N_CATEGORY['TAGS']\n",
    "label_type = 'obstacle'\n",
    "gsv_not_pannellum = False\n",
    "\n",
    "image_dimension = 256\n",
    "\n",
    "base_model_size = 'base'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:18.457570Z",
     "start_time": "2024-04-03T15:45:18.451429Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "ztcDoPyIVF28",
    "outputId": "df80bf6d-9d67-4486-a582-02ec5dff7143",
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:18.466329Z",
     "start_time": "2024-04-03T15:45:18.458631Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are settings for ensuring input images to DinoV2 are properly sized\n",
    "\n",
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Resize the image\n",
    "        img = transforms.Resize(self.target_size)(img)\n",
    "\n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        img = transforms.Pad((pad_width // 2, pad_height // 2, pad_width - pad_width // 2, pad_height - pad_height // 2))(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "# This is what DinoV2 sees\n",
    "target_size = (image_dimension, image_dimension)\n",
    "\n",
    "# Below are functions that every image will be passed through, including data augmentations\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            ResizeAndPad(target_size, 14),\n",
    "            # transforms.RandomRotation(360),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            # transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"inference\": transforms.Compose([ ResizeAndPad(target_size, 14),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                             ]\n",
    "                                            )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pGzRW8J1VF29",
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:20.023923Z",
     "start_time": "2024-04-03T15:45:18.467534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_target_classes(dir_path, offset):\n",
    "    file_path = os.path.join(dir_path, '_classes.csv')\n",
    "    data = pd.read_csv(file_path)\n",
    "    header_row = data.columns.tolist()\n",
    "    return header_row[offset:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:20.030865Z",
     "start_time": "2024-04-03T15:45:20.024934Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Comet tracking\n",
    "# experiment = Experiment(\n",
    "#   api_key=\"ACp1vdQWhJgzUu6Svb9vcKyPH\",\n",
    "#   project_name=\"ps-tags\",\n",
    "#   workspace=\"hoominchu\"\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:20.037699Z",
     "start_time": "2024-04-03T15:45:20.031966Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if gsv_not_pannellum:\n",
    "    csv_data_dir_path_train = '../datasets/crops-' + label_type + '-' + c12n_category + '/train'\n",
    "    model_name_to_save = 'cls-' + base_model_size[0] + '-' + label_type + '-' + c12n_category + '-best.pth'\n",
    "else:\n",
    "    csv_data_dir_path_train = '../datasets/crops-' + label_type + '-' + c12n_category + '-pannellum' + '/train'\n",
    "    model_name_to_save = 'cls-' + base_model_size[0] + '-' + label_type + '-' + c12n_category + '-pannellum-best.pth'\n",
    "\n",
    "\n",
    "\n",
    "# we will pass this to the model, so we don't have to change it manually\n",
    "n_target_classes = len(get_target_classes(csv_data_dir_path_train, c12n_category_offset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:20.050730Z",
     "start_time": "2024-04-03T15:45:20.038808Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CSMLRlDUVF2-",
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:20.805546Z",
     "start_time": "2024-04-03T15:45:20.051886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new classifier layer that contains a few linear layers with a ReLU to make predictions positive\n",
    "class DinoVisionTransformerClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, model_size=\"small\", nc=1):\n",
    "        super(DinoVisionTransformerClassifier, self).__init__()\n",
    "        self.model_size = model_size\n",
    "\n",
    "        # loading a model with registers\n",
    "        n_register_tokens = 4\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            model = vit_small(patch_size=14,\n",
    "                              img_size=526,\n",
    "                              init_values=1.0,\n",
    "                              num_register_tokens=n_register_tokens,\n",
    "                              block_chunks=0)\n",
    "            self.embedding_size = 384\n",
    "            self.number_of_heads = 6\n",
    "\n",
    "        elif model_size == \"base\":\n",
    "            model = vit_base(patch_size=14,\n",
    "                             img_size=526,\n",
    "                             init_values=1.0,\n",
    "                             num_register_tokens=n_register_tokens,\n",
    "                             block_chunks=0)\n",
    "            self.embedding_size = 768\n",
    "            self.number_of_heads = 12\n",
    "\n",
    "        elif model_size == \"large\":\n",
    "            model = vit_large(patch_size=14,\n",
    "                              img_size=526,\n",
    "                              init_values=1.0,\n",
    "                              num_register_tokens=n_register_tokens,\n",
    "                              block_chunks=0)\n",
    "            self.embedding_size = 1024\n",
    "            self.number_of_heads = 16\n",
    "\n",
    "        elif model_size == \"giant\":\n",
    "            model = vit_giant2(patch_size=14,\n",
    "                               img_size=526,\n",
    "                               init_values=1.0,\n",
    "                               num_register_tokens=n_register_tokens,\n",
    "                               block_chunks=0)\n",
    "            self.embedding_size = 1536\n",
    "            self.number_of_heads = 24\n",
    "\n",
    "        # Download pre-trained weights and place locally as-needed:\n",
    "        # - small: https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth\n",
    "        # - base:  https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth\n",
    "        # - large: https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth\n",
    "        # - giant: https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth\n",
    "        model.load_state_dict(torch.load(Path('{}/../dinov2_vit{}14_reg4_pretrain.pth'.format(local_directory, base_model_size[0]))))\n",
    "\n",
    "        self.transformer = deepcopy(model)\n",
    "\n",
    "\n",
    "        # @zhihan, question: should the 256 be the same as the image resolution? or does it not matter?\n",
    "        self.classifier = nn.Sequential(nn.Linear(self.embedding_size, 256), nn.ReLU(), nn.Linear(256, nc))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.transformer.norm(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = DinoVisionTransformerClassifier(base_model_size, n_target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vs84EaOQVF2_",
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:20.943120Z",
     "start_time": "2024-04-03T15:45:20.806574Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.train()\n",
    "# change the binary cross-entropy loss below to a different loss if using more than 2 classes\n",
    "# https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "# @zhihan, question: are these loss functions correct?\n",
    "if c12n_category == C12N_CATEGORY['TAGS']:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif c12n_category == C12N_CATEGORY['SEVERITY']:\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "hyper_params = {\n",
    "   \"learning_rate\": '1e-6',\n",
    "   \"steps\": num_epochs,\n",
    "   \"batch_size\": 1,\n",
    "    # add other hyper params\n",
    "}\n",
    "\n",
    "# experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# custom data loader\n",
    "class PSLabelsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, offset=3, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            offset (int, optional): number of columns to skip to get to the severity or tags info e.g. normalized_x and normalized_y need to be skipped.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.offset = offset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.read_image(img_name, mode=io.ImageReadMode.RGB)\n",
    "        labels = self.landmarks_frame.iloc[idx, self.offset:]\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.astype('float')\n",
    "        # sample = {'image': image, 'labels': labels}\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:20.948034Z",
     "start_time": "2024-04-03T15:45:20.943120Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = PSLabelsDataset(csv_data_dir_path_train + '/' + '_classes.csv', root_dir=csv_data_dir_path_train, offset=c12n_category_offset, transform=data_transforms['train'])\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T15:45:20.975943Z",
     "start_time": "2024-04-03T15:45:20.949038Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14vdJZdKVF3A",
    "outputId": "e1fce9ee-a366-4fc3-cebb-0782807ba3f1",
    "scrolled": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-03T15:45:33.357342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "  -> Epoch 0: Loss = 0.31650, Accuracy = 90.324%\n",
      "Saving model based on accuracy: 0.31650 | Accuracy: 90.324%\n",
      "  -> Epoch 1: Loss = 0.19096, Accuracy = 94.521%\n",
      "Saving model based on accuracy: 0.19096 | Accuracy: 94.521%\n",
      "  -> Epoch 2: Loss = 0.16627, Accuracy = 95.250%\n",
      "Saving model based on accuracy: 0.16627 | Accuracy: 95.250%\n",
      "  -> Epoch 3: Loss = 0.14654, Accuracy = 95.874%\n",
      "Saving model based on accuracy: 0.14654 | Accuracy: 95.874%\n",
      "  -> Epoch 4: Loss = 0.12947, Accuracy = 96.585%\n",
      "Saving model based on accuracy: 0.12947 | Accuracy: 96.585%\n",
      "  -> Epoch 5: Loss = 0.11430, Accuracy = 97.097%\n",
      "Saving model based on accuracy: 0.11430 | Accuracy: 97.097%\n",
      "  -> Epoch 6: Loss = 0.10040, Accuracy = 97.488%\n",
      "Saving model based on accuracy: 0.10040 | Accuracy: 97.488%\n",
      "  -> Epoch 7: Loss = 0.08816, Accuracy = 97.912%\n",
      "Saving model based on accuracy: 0.08816 | Accuracy: 97.912%\n",
      "  -> Epoch 8: Loss = 0.07709, Accuracy = 98.312%\n",
      "Saving model based on accuracy: 0.07709 | Accuracy: 98.312%\n",
      "  -> Epoch 9: Loss = 0.06706, Accuracy = 98.629%\n",
      "Saving model based on accuracy: 0.06706 | Accuracy: 98.629%\n",
      "  -> Epoch 10: Loss = 0.05878, Accuracy = 98.841%\n",
      "Saving model based on accuracy: 0.05878 | Accuracy: 98.841%\n",
      "  -> Epoch 11: Loss = 0.05150, Accuracy = 98.956%\n",
      "Saving model based on accuracy: 0.05150 | Accuracy: 98.956%\n",
      "  -> Epoch 12: Loss = 0.04543, Accuracy = 99.068%\n",
      "Saving model based on accuracy: 0.04543 | Accuracy: 99.068%\n",
      "  -> Epoch 13: Loss = 0.03992, Accuracy = 99.197%\n",
      "Saving model based on accuracy: 0.03992 | Accuracy: 99.197%\n",
      "  -> Epoch 14: Loss = 0.03486, Accuracy = 99.321%\n",
      "Saving model based on accuracy: 0.03486 | Accuracy: 99.321%\n",
      "  -> Epoch 15: Loss = 0.03059, Accuracy = 99.450%\n",
      "Saving model based on accuracy: 0.03059 | Accuracy: 99.450%\n",
      "  -> Epoch 16: Loss = 0.02673, Accuracy = 99.603%\n",
      "Saving model based on accuracy: 0.02673 | Accuracy: 99.603%\n",
      "  -> Epoch 17: Loss = 0.02380, Accuracy = 99.703%\n",
      "Saving model based on accuracy: 0.02380 | Accuracy: 99.703%\n",
      "  -> Epoch 18: Loss = 0.02096, Accuracy = 99.782%\n",
      "Saving model based on accuracy: 0.02096 | Accuracy: 99.782%\n",
      "  -> Epoch 19: Loss = 0.01882, Accuracy = 99.815%\n",
      "Saving model based on accuracy: 0.01882 | Accuracy: 99.815%\n",
      "  -> Epoch 20: Loss = 0.01711, Accuracy = 99.832%\n",
      "Saving model based on accuracy: 0.01711 | Accuracy: 99.832%\n",
      "  -> Epoch 21: Loss = 0.01517, Accuracy = 99.865%\n",
      "Saving model based on accuracy: 0.01517 | Accuracy: 99.865%\n",
      "  -> Epoch 22: Loss = 0.01340, Accuracy = 99.894%\n",
      "Saving model based on accuracy: 0.01340 | Accuracy: 99.894%\n",
      "  -> Epoch 23: Loss = 0.01196, Accuracy = 99.909%\n",
      "Saving model based on accuracy: 0.01196 | Accuracy: 99.909%\n",
      "  -> Epoch 24: Loss = 0.01085, Accuracy = 99.935%\n",
      "Saving model based on accuracy: 0.01085 | Accuracy: 99.935%\n",
      "  -> Epoch 25: Loss = 0.00998, Accuracy = 99.935%\n",
      "Saving model based on loss: 0.00998 | Accuracy: 99.935%\n",
      "  -> Epoch 26: Loss = 0.00921, Accuracy = 99.932%\n",
      "  -> Epoch 27: Loss = 0.00826, Accuracy = 99.959%\n",
      "Saving model based on accuracy: 0.00826 | Accuracy: 99.959%\n",
      "  -> Epoch 28: Loss = 0.00762, Accuracy = 99.962%\n",
      "Saving model based on accuracy: 0.00762 | Accuracy: 99.962%\n",
      "  -> Epoch 29: Loss = 0.00665, Accuracy = 99.982%\n",
      "Saving model based on accuracy: 0.00665 | Accuracy: 99.982%\n",
      "  -> Epoch 30: Loss = 0.00615, Accuracy = 99.982%\n",
      "Saving model based on loss: 0.00615 | Accuracy: 99.982%\n",
      "  -> Epoch 31: Loss = 0.00569, Accuracy = 99.982%\n",
      "Saving model based on loss: 0.00569 | Accuracy: 99.982%\n",
      "  -> Epoch 32: Loss = 0.00537, Accuracy = 99.979%\n",
      "  -> Epoch 33: Loss = 0.00495, Accuracy = 99.982%\n",
      "Saving model based on loss: 0.00495 | Accuracy: 99.982%\n",
      "  -> Epoch 34: Loss = 0.00432, Accuracy = 99.997%\n",
      "Saving model based on accuracy: 0.00432 | Accuracy: 99.997%\n",
      "  -> Epoch 35: Loss = 0.00396, Accuracy = 99.994%\n",
      "  -> Epoch 36: Loss = 0.00562, Accuracy = 99.938%\n",
      "  -> Epoch 37: Loss = 0.00344, Accuracy = 100.000%\n",
      "Saving model based on accuracy: 0.00344 | Accuracy: 100.000%\n",
      "  -> Epoch 38: Loss = 0.00297, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00297 | Accuracy: 100.000%\n",
      "  -> Epoch 39: Loss = 0.00270, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00270 | Accuracy: 100.000%\n",
      "  -> Epoch 40: Loss = 0.00251, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00251 | Accuracy: 100.000%\n",
      "  -> Epoch 41: Loss = 0.00238, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00238 | Accuracy: 100.000%\n",
      "  -> Epoch 42: Loss = 0.00243, Accuracy = 99.997%\n",
      "  -> Epoch 43: Loss = 0.00327, Accuracy = 99.976%\n",
      "  -> Epoch 44: Loss = 0.00454, Accuracy = 99.932%\n",
      "  -> Epoch 45: Loss = 0.00257, Accuracy = 99.991%\n",
      "  -> Epoch 46: Loss = 0.00189, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00189 | Accuracy: 100.000%\n",
      "  -> Epoch 47: Loss = 0.00169, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00169 | Accuracy: 100.000%\n",
      "  -> Epoch 48: Loss = 0.00156, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00156 | Accuracy: 100.000%\n",
      "  -> Epoch 49: Loss = 0.00145, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00145 | Accuracy: 100.000%\n",
      "  -> Epoch 50: Loss = 0.00134, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00134 | Accuracy: 100.000%\n",
      "  -> Epoch 51: Loss = 0.00128, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00128 | Accuracy: 100.000%\n",
      "  -> Epoch 52: Loss = 0.00131, Accuracy = 99.997%\n",
      "  -> Epoch 53: Loss = 0.00521, Accuracy = 99.900%\n",
      "  -> Epoch 54: Loss = 0.00190, Accuracy = 99.988%\n",
      "  -> Epoch 55: Loss = 0.00128, Accuracy = 100.000%\n",
      "Saving model based on loss: 0.00128 | Accuracy: 100.000%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# accuracy calculation for multilabel i.e. tags\n",
    "def calculate_accuracy_multilabel(outputs, labels):\n",
    "    # Convert outputs to probabilities using sigmoid\n",
    "    probabilities = torch.sigmoid(outputs)\n",
    "    # Convert probabilities to predicted classes\n",
    "    predicted_classes = probabilities > 0.5\n",
    "    # Calculate accuracy\n",
    "    n_labels = labels.size(1)\n",
    "    correct_predictions = ((predicted_classes == labels.byte()).sum().item()) / n_labels\n",
    "    \n",
    "    total_predictions = labels.size(0)\n",
    "    \n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "# accuracy calculation for multiclass i.e. severity\n",
    "def calculate_accuracy_multiclass(outputs, labels):\n",
    "    # Convert outputs to probabilities using sigmoid\n",
    "    probabilities = torch.softmax(outputs, 1)\n",
    "    # Convert probabilities to predicted classes\n",
    "    predicted_class_idx = torch.argmax(probabilities, 1).item()\n",
    "\n",
    "    return int(labels[0][predicted_class_idx].item())\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "\n",
    "best_accuracy = 0\n",
    "best_loss = 100\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "\n",
    "    for data in dataloaders[\"train\"]:\n",
    "        # get the input batch and the ground truth labels\n",
    "        batch_of_images, gt_labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # model prediction\n",
    "        output = model(batch_of_images.to(device)).squeeze(dim=1)\n",
    "        \n",
    "        # if c12n_category == C12N_CATEGORY['TAGS']:\n",
    "        #     output = torch.sigmoid(output)\n",
    "        \n",
    "        if c12n_category == C12N_CATEGORY['SEVERITY']:\n",
    "            output = torch.softmax(output, 1)\n",
    "\n",
    "        # compute loss and do gradient descent\n",
    "        loss = criterion(output, gt_labels.float().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        # Calculate and record batch accuracy\n",
    "        \n",
    "        accuracy = 0\n",
    "        \n",
    "        if c12n_category == C12N_CATEGORY['TAGS']:\n",
    "            accuracy = calculate_accuracy_multilabel(output, gt_labels.to(device))\n",
    "        elif c12n_category == C12N_CATEGORY['SEVERITY']:\n",
    "            accuracy = calculate_accuracy_multiclass(output, gt_labels.to(device))\n",
    "        \n",
    "        batch_accuracies.append(accuracy)\n",
    "\n",
    "    epoch_losses.append(np.mean(batch_losses))\n",
    "    epoch_accuracy = np.mean(batch_accuracies)\n",
    "    epoch_loss = epoch_losses[-1]\n",
    "    epoch_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    print(\"  -> Epoch {}: Loss = {:.5f}, Accuracy = {:.3f}%\".format(epoch, epoch_losses[-1], 100*epoch_accuracy))\n",
    "    \n",
    "    # save the model if it has the best accuracy so far\n",
    "    # @zhihan, should we be checking for accuracy at all? or should we just consider the model with the lowest loss as the best? \n",
    "    if epoch_accuracy > best_accuracy:\n",
    "        best_accuracy = epoch_accuracy\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), '{}/'.format(local_directory) + model_name_to_save)\n",
    "        print('Saving model based on accuracy: {:.5f} | Accuracy: {:.3f}%'.format(best_loss, 100*best_accuracy))\n",
    "    elif epoch_accuracy == best_accuracy:\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_accuracy = epoch_accuracy\n",
    "            torch.save(model.state_dict(), '{}/'.format(local_directory) + model_name_to_save)\n",
    "            print('Saving model based on loss: {:.5f} | Accuracy: {:.3f}%'.format(best_loss, 100*best_accuracy))\n",
    "            \n",
    "    # # track on comet ml        \n",
    "    # log_model(experiment, model_name_to_save, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JS7qHekbVF3B",
    "outputId": "cc9c8113-15df-4846-ff94-2726c81a82ec",
    "ExecuteTime": {
     "end_time": "2024-04-03T15:44:35.645811Z",
     "start_time": "2024-04-03T15:44:35.396824Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Plotting accuracy\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39msubplot(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(epoch_accuracies, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblue\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_accuracies, label='Accuracy', color='blue')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_losses, label='Loss', color='red')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
