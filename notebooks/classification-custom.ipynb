{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook trains a classification model to predict 'tags' or 'severity' for the given Project Sidewalk label type."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0XmqhrS1VF2g",
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:31.043084Z",
     "start_time": "2024-03-21T00:21:31.038537Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "from torchvision import transforms, io\n",
    "from dinov2.models.vision_transformer import vit_small, vit_base, vit_large, vit_giant2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# some initial variables\n",
    "\n",
    "local_directory = os.getcwd()\n",
    "\n",
    "# Enum for the classification categories we support.\n",
    "C12N_CATEGORY = {\n",
    "    'TAGS': 'tags',\n",
    "    'SEVERITY': 'severity'\n",
    "}\n",
    "\n",
    "# this has to be consistent with the data generation script\n",
    "c12n_category_offset = 3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:31.050705Z",
     "start_time": "2024-03-21T00:21:31.044283Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# All the parameters that need to be configured for a training run should be in this cell.\n",
    "# Everywhere else we will use these variables.\n",
    "\n",
    "# classification category. currently, one of 'severity' or 'tags'.\n",
    "c12n_category = C12N_CATEGORY['TAGS']\n",
    "label_type = 'surfaceproblem'\n",
    "\n",
    "image_dimension = 512\n",
    "\n",
    "base_model_size = 'large'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:31.058112Z",
     "start_time": "2024-03-21T00:21:31.051860Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "ztcDoPyIVF28",
    "outputId": "df80bf6d-9d67-4486-a582-02ec5dff7143",
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:31.066009Z",
     "start_time": "2024-03-21T00:21:31.059120Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are settings for ensuring input images to DinoV2 are properly sized\n",
    "\n",
    "class ResizeAndPad:\n",
    "    def __init__(self, target_size, multiple):\n",
    "        self.target_size = target_size\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Resize the image\n",
    "        img = transforms.Resize(self.target_size)(img)\n",
    "\n",
    "        # Calculate padding\n",
    "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
    "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
    "\n",
    "        # Apply padding\n",
    "        img = transforms.Pad((pad_width // 2, pad_height // 2, pad_width - pad_width // 2, pad_height - pad_height // 2))(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "# This is what DinoV2 sees\n",
    "target_size = (image_dimension, image_dimension)\n",
    "\n",
    "# Below are functions that every image will be passed through, including data augmentations\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            ResizeAndPad(target_size, 14),\n",
    "            # transforms.RandomRotation(360),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            # transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"inference\": transforms.Compose([ ResizeAndPad(target_size, 14),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                             ]\n",
    "                                            )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pGzRW8J1VF29",
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:31.072650Z",
     "start_time": "2024-03-21T00:21:31.067013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_target_classes(dir_path, offset):\n",
    "    file_path = os.path.join(dir_path, '_classes.csv')\n",
    "    data = pd.read_csv(file_path)\n",
    "    header_row = data.columns.tolist()\n",
    "    return header_row[offset:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:31.079762Z",
     "start_time": "2024-03-21T00:21:31.073693Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Comet tracking\n",
    "# experiment = Experiment(\n",
    "#   api_key=\"ACp1vdQWhJgzUu6Svb9vcKyPH\",\n",
    "#   project_name=\"ps-tags\",\n",
    "#   workspace=\"hoominchu\"\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:31.086639Z",
     "start_time": "2024-03-21T00:21:31.080765Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "valid_dataset_dir = Path(\"../datasets/crops-\" + label_type + \"-\" + c12n_category + \"/valid\")   # in \"val\", same as above\n",
    "csv_data_dir_path_train = '../datasets/crops-' + label_type + '-' + c12n_category + '/train'\n",
    "\n",
    "model_name_to_save = 'cls-' + base_model_size[0] + '-' + label_type + '-' + c12n_category + '-best.pth'\n",
    "\n",
    "# we will pass this to the model, so we don't have to change it manually\n",
    "n_target_classes = len(get_target_classes(csv_data_dir_path_train, c12n_category_offset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:31.098103Z",
     "start_time": "2024-03-21T00:21:31.087640Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CSMLRlDUVF2-",
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:34.074824Z",
     "start_time": "2024-03-21T00:21:31.099163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new classifier layer that contains a few linear layers with a ReLU to make predictions positive\n",
    "class DinoVisionTransformerClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, model_size=\"small\", nc=1):\n",
    "        super(DinoVisionTransformerClassifier, self).__init__()\n",
    "        self.model_size = model_size\n",
    "\n",
    "        # loading a model with registers\n",
    "        n_register_tokens = 4\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            model = vit_small(patch_size=14,\n",
    "                              img_size=526,\n",
    "                              init_values=1.0,\n",
    "                              num_register_tokens=n_register_tokens,\n",
    "                              block_chunks=0)\n",
    "            self.embedding_size = 384\n",
    "            self.number_of_heads = 6\n",
    "\n",
    "        elif model_size == \"base\":\n",
    "            model = vit_base(patch_size=14,\n",
    "                             img_size=526,\n",
    "                             init_values=1.0,\n",
    "                             num_register_tokens=n_register_tokens,\n",
    "                             block_chunks=0)\n",
    "            self.embedding_size = 768\n",
    "            self.number_of_heads = 12\n",
    "\n",
    "        elif model_size == \"large\":\n",
    "            model = vit_large(patch_size=14,\n",
    "                              img_size=526,\n",
    "                              init_values=1.0,\n",
    "                              num_register_tokens=n_register_tokens,\n",
    "                              block_chunks=0)\n",
    "            self.embedding_size = 1024\n",
    "            self.number_of_heads = 16\n",
    "\n",
    "        elif model_size == \"giant\":\n",
    "            model = vit_giant2(patch_size=14,\n",
    "                               img_size=526,\n",
    "                               init_values=1.0,\n",
    "                               num_register_tokens=n_register_tokens,\n",
    "                               block_chunks=0)\n",
    "            self.embedding_size = 1536\n",
    "            self.number_of_heads = 24\n",
    "\n",
    "        # Download pre-trained weights and place locally as-needed:\n",
    "        # - small: https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth\n",
    "        # - base:  https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth\n",
    "        # - large: https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth\n",
    "        # - giant: https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth\n",
    "        model.load_state_dict(torch.load(Path('{}/../dinov2_vit{}14_reg4_pretrain.pth'.format(local_directory, base_model_size[0]))))\n",
    "\n",
    "        self.transformer = deepcopy(model)\n",
    "\n",
    "\n",
    "        # @zhihan, question: should the 256 be the same as the image resolution? or does it not matter?\n",
    "        self.classifier = nn.Sequential(nn.Linear(self.embedding_size, 256), nn.ReLU(), nn.Linear(256, nc))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.transformer.norm(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = DinoVisionTransformerClassifier(base_model_size, n_target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vs84EaOQVF2_",
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:34.450848Z",
     "start_time": "2024-03-21T00:21:34.074824Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 24\u001B[0m\n\u001B[0;32m     15\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m     17\u001B[0m hyper_params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     18\u001B[0m    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1e-6\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     19\u001B[0m    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m\"\u001B[39m: num_epochs,\n\u001B[0;32m     20\u001B[0m    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# add other hyper params\u001B[39;00m\n\u001B[0;32m     22\u001B[0m }\n\u001B[1;32m---> 24\u001B[0m experiment\u001B[38;5;241m.\u001B[39mlog_parameters(hyper_params)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model = model.train()\n",
    "# change the binary cross-entropy loss below to a different loss if using more than 2 classes\n",
    "# https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "# @zhihan, question: are these loss functions correct?\n",
    "if c12n_category == C12N_CATEGORY['TAGS']:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "elif c12n_category == C12N_CATEGORY['SEVERITY']:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "hyper_params = {\n",
    "   \"learning_rate\": '1e-6',\n",
    "   \"steps\": num_epochs,\n",
    "   \"batch_size\": 1,\n",
    "    # add other hyper params\n",
    "}\n",
    "\n",
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# custom data loader\n",
    "class PSLabelsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, offset=3, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            offset (int, optional): number of columns to skip to get to the severity or tags info e.g. normalized_x and normalized_y need to be skipped.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.offset = offset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.read_image(img_name, mode=io.ImageReadMode.RGB)\n",
    "        labels = self.landmarks_frame.iloc[idx, self.offset:]\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.astype('float')\n",
    "        # sample = {'image': image, 'labels': labels}\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T00:21:34.451989Z",
     "start_time": "2024-03-21T00:21:34.451989Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = PSLabelsDataset(csv_data_dir_path_train + '/' + '_classes.csv', root_dir=csv_data_dir_path_train, offset=c12n_category_offset, transform=data_transforms['train'])\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14vdJZdKVF3A",
    "outputId": "e1fce9ee-a366-4fc3-cebb-0782807ba3f1",
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2024-03-21T00:21:34.453106Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# accuracy calculation for multilabel i.e. tags\n",
    "def calculate_accuracy_multilabel(outputs, labels):\n",
    "    # Convert outputs to probabilities using sigmoid\n",
    "    probabilities = torch.sigmoid(outputs)\n",
    "    # Convert probabilities to predicted classes\n",
    "    predicted_classes = probabilities > 0.5\n",
    "    # Calculate accuracy\n",
    "    n_labels = labels.size(1)\n",
    "    correct_predictions = ((predicted_classes == labels.byte()).sum().item()) / n_labels\n",
    "    \n",
    "    total_predictions = labels.size(0)\n",
    "    \n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "# accuracy calculation for multiclass i.e. severity\n",
    "def calculate_accuracy_multiclass(outputs, labels):\n",
    "    # Convert outputs to probabilities using sigmoid\n",
    "    probabilities = torch.softmax(outputs, 1)\n",
    "    # Convert probabilities to predicted classes\n",
    "    predicted_class_idx = torch.argmax(probabilities, 1).item()\n",
    "\n",
    "    return int(labels[0][predicted_class_idx].item())\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "\n",
    "best_accuracy = 0\n",
    "best_loss = 100\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "\n",
    "    for data in dataloaders[\"train\"]:\n",
    "        # get the input batch and the ground truth labels\n",
    "        batch_of_images, gt_labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # model prediction\n",
    "        output = model(batch_of_images.to(device)).squeeze(dim=1)\n",
    "        \n",
    "        # if c12n_category == C12N_CATEGORY['TAGS']:\n",
    "        #     output = torch.sigmoid(output)\n",
    "        \n",
    "        if c12n_category == C12N_CATEGORY['SEVERITY']:\n",
    "            output = torch.softmax(output, 1)\n",
    "\n",
    "        # compute loss and do gradient descent\n",
    "        loss = criterion(output, gt_labels.float().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        # Calculate and record batch accuracy\n",
    "        \n",
    "        accuracy = 0\n",
    "        \n",
    "        if c12n_category == C12N_CATEGORY['TAGS']:\n",
    "            accuracy = calculate_accuracy_multilabel(output, gt_labels.to(device))\n",
    "        elif c12n_category == C12N_CATEGORY['SEVERITY']:\n",
    "            accuracy = calculate_accuracy_multiclass(output, gt_labels.to(device))\n",
    "        \n",
    "        batch_accuracies.append(accuracy)\n",
    "\n",
    "    epoch_losses.append(np.mean(batch_losses))\n",
    "    epoch_accuracy = np.mean(batch_accuracies)\n",
    "    epoch_loss = epoch_losses[-1]\n",
    "    epoch_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    print(\"  -> Epoch {}: Loss = {:.5f}, Accuracy = {:.3f}%\".format(epoch, epoch_losses[-1], 100*epoch_accuracy))\n",
    "    \n",
    "    # save the model if it has the best accuracy so far\n",
    "    # @zhihan, should we be checking for accuracy at all? or should we just consider the model with the lowest loss as the best? \n",
    "    if epoch_accuracy > best_accuracy:\n",
    "        best_accuracy = epoch_accuracy\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), '{}/'.format(local_directory) + model_name_to_save)\n",
    "        print('Saving model based on accuracy: {:.5f} | Accuracy: {:.3f}%'.format(best_loss, 100*best_accuracy))\n",
    "    elif epoch_accuracy == best_accuracy:\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_accuracy = epoch_accuracy\n",
    "            torch.save(model.state_dict(), '{}/'.format(local_directory) + model_name_to_save)\n",
    "            print('Saving model based on loss: {:.5f} | Accuracy: {:.3f}%'.format(best_loss, 100*best_accuracy))\n",
    "            \n",
    "    # # track on comet ml        \n",
    "    # log_model(experiment, model_name_to_save, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JS7qHekbVF3B",
    "outputId": "cc9c8113-15df-4846-ff94-2726c81a82ec",
    "ExecuteTime": {
     "start_time": "2024-03-21T00:21:34.454216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_accuracies, label='Accuracy', color='blue')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_losses, label='Loss', color='red')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
